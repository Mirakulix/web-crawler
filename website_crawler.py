#!/usr/bin/env python3
"""
Website Crawler und Archiver
Crawlt eine Website und speichert alle Inhalte als Text und PDF
Mit Login-Unterst√ºtzung und PDF-Fusion-Option
"""

import os
import re
import json
import requests
from urllib.parse import urljoin, urlparse, quote
from urllib.robotparser import RobotFileParser
from pathlib import Path
import time
from typing import Set, List, Dict, Optional, Tuple
from dataclasses import dataclass, asdict
from collections import defaultdict
import logging
import getpass  # F√ºr versteckte Passwort-Eingabe
from enum import Enum

class CrawlScope(Enum):
    """Definiert den Umfang des Crawlings"""
    HIERARCHICAL = "hierarchical"  # Nur unter/neben der Basis-URL
    DOMAIN_ONLY = "domain_only"    # Alle URLs der gleichen Domain
    ALL_URLS = "all_urls"          # Alle URLs ohne Einschr√§nkung

# Third-party imports
try:
    from bs4 import BeautifulSoup
    from weasyprint import HTML, CSS
    import base64
    from PIL import Image
    from PyPDF2 import PdfMerger  # F√ºr PDF-Fusion
except ImportError as e:
    print(f"Fehler: Ben√∂tigte Bibliotheken nicht installiert: {e}")
    print("Bitte installieren Sie: pip install beautifulsoup4 weasyprint requests pillow PyPDF2")
    print("Zus√§tzlich f√ºr WeasyPrint: sudo apt-get install libpango-1.0-0 libharfbuzz0b libpangoft2-1.0-0 (Linux)")
    print("Oder: brew install pango (macOS)")
    exit(1)

# Logging Setup
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('crawler.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Debug-Modus aktivieren falls gew√ºnscht
DEBUG_MODE = False

def enable_debug():
    """Aktiviert Debug-Logging"""
    global DEBUG_MODE
    DEBUG_MODE = True
    logger.setLevel(logging.DEBUG)
    logging.getLogger().setLevel(logging.DEBUG)
    logger.debug("Debug-Modus aktiviert")

@dataclass
class WebPage:
    """Datenklasse f√ºr eine Webseite"""
    url: str
    title: str
    content: str
    images: List[Dict[str, str]]  # Ge√§ndert zu Dictionary mit Metadaten
    links: List[Tuple[str, str]]  # (display_text, url)
    file_path: str
    pdf_path: str
    timestamp: str

class WebsiteCrawler:
    """Haupt-Crawler-Klasse"""
    
    def __init__(self, base_url: str, max_urls: int, single_pdf: bool = False, crawl_scope: CrawlScope = CrawlScope.HIERARCHICAL):
        self.base_url = base_url.rstrip('/')
        self.max_urls = max_urls
        self.single_pdf = single_pdf
        self.crawl_scope = crawl_scope
        self.visited_urls: Set[str] = set()
        self.pages: List[WebPage] = []
        self.queue: List[str] = [base_url]
        
        # Login-Credentials (nur f√ºr diese Session)
        self.login_credentials: Dict[str, Dict[str, str]] = {}
        self.login_attempted: Set[str] = set()
        
        # Basis-Domain und Pfad f√ºr Filterung
        parsed_base = urlparse(base_url)
        self.base_domain = parsed_base.netloc
        self.base_path = parsed_base.path
        
        # Ordner erstellen
        self.output_dir = self._create_output_directory()
        self.images_dir = self.output_dir / "images"
        self.texts_dir = self.output_dir / "texts"  # Neuer Texts-Ordner
        self.images_dir.mkdir(exist_ok=True)
        self.texts_dir.mkdir(exist_ok=True)
        
        # Session f√ºr effiziente Requests
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # PDF-Pfade f√ºr eventuelle Fusion
        self.pdf_paths: List[str] = []
        
        # Statistiken
        self.stats = {
            'processed': 0,
            'errors': 0,
            'images_downloaded': 0,
            'pagination_links_found': 0,
            'total_links_found': 0,
            'skipped_external': 0,
            'skipped_invalid': 0,
            'login_pages_found': 0,
            'login_attempts': 0,
            'login_successes': 0,
            'crawl_scope': crawl_scope.value
        }
    
    def _create_output_directory(self) -> Path:
        """Erstellt den Ausgabeordner mit korrektem Namen"""
        # URL zu g√ºltigem Ordnernamen konvertieren
        dir_name = self.base_url.replace('https://', '').replace('http://', '').replace('www.','').rstrip('/')
        dir_name = dir_name.replace('/', '-').replace('.', '_').strip("-")
        
        dir_name = re.sub(r'[<>:"/\\|?*]', '_', dir_name)
        output_dir = input(f"Der Ausgabeordner wird {dir_name} benannt, lieber einen eigenen Namen eingeben?\n").strip() or dir_name
        if output_dir.startswith('/'):
            output_dir = Path(output_dir)
        elif output_dir.startswith('./'):
            output_dir = Path(os.path.join(os.getcwd(), output_dir[2:]))
        elif "/" or "\\" in output_dir and not output_dir.startswith("/"):
            output_dir = Path(os.path.join(os.getcwd(), output_dir))
        else:
            output_dir = Path(os.path.join(os.getcwd(), output_dir))
        
        logger.info(f"Erstelle Ausgabeordner: {output_dir}")
        output_dir.mkdir(exist_ok=True)
        logger.info(f"Ausgabeordner erstellt: {output_dir}")
        return output_dir
    
    def _is_valid_url(self, url: str) -> bool:
        """Pr√ºft, ob eine URL verarbeitet werden soll basierend auf dem Crawl-Scope"""
        try:
            parsed = urlparse(url)
            
            # Bestimmte Dateitypen immer ausschlie√üen
            excluded_extensions = {'.pdf', '.jpg', '.jpeg', '.png', '.gif', '.zip', '.rar', '.exe', '.doc', '.docx', '.xls', '.xlsx'}
            if any(parsed.path.lower().endswith(ext) for ext in excluded_extensions):
                logger.debug(f"URL {url} ausgeschlossen: Dateierweiterung")
                return False
            
            # Bestimmte URL-Parameter ausschlie√üen (z.B. Logout-Links)
            excluded_params = {'logout', 'signout', 'exit', 'delete', 'remove'}
            if any(param in parsed.query.lower() for param in excluded_params):
                logger.debug(f"URL {url} ausgeschlossen: ausgeschlossene Parameter")
                return False
            
            # Je nach Crawl-Scope unterschiedliche Validierung
            if self.crawl_scope == CrawlScope.ALL_URLS:
                # Alle URLs sind erlaubt (au√üer die oben ausgeschlossenen)
                logger.debug(f"URL {url} erlaubt: ALL_URLS Modus")
                return True
                
            elif self.crawl_scope == CrawlScope.DOMAIN_ONLY:
                # Nur URLs der gleichen Domain
                if parsed.netloc != self.base_domain:
                    logger.debug(f"URL {url} ausgeschlossen: andere Domain {parsed.netloc} vs {self.base_domain}")
                    return False
                logger.debug(f"URL {url} erlaubt: gleiche Domain")
                return True
                
            elif self.crawl_scope == CrawlScope.HIERARCHICAL:
                # Hierarchische Validierung (urspr√ºngliches Verhalten)
                
                # Externe Domains ausschlie√üen
                if parsed.netloc != self.base_domain:
                    logger.debug(f"URL {url} ausgeschlossen: externe Domain {parsed.netloc}")
                    return False
                
                # URLs die hierarchisch unter oder neben der Basis-URL liegen sind erlaubt
                base_path_parts = [part for part in self.base_path.split('/') if part]
                url_path_parts = [part for part in parsed.path.split('/') if part]
                
                logger.debug(f"URL-Validation: {url}")
                logger.debug(f"Base path parts: {base_path_parts}")
                logger.debug(f"URL path parts: {url_path_parts}")
                
                # √úberpr√ºfen ob URL unter oder neben der Basis-URL liegt
                if len(url_path_parts) < len(base_path_parts):
                    # URL ist hierarchisch √ºber der Basis-URL -> ausschlie√üen
                    logger.debug(f"URL {url} ausgeschlossen: hierarchisch √ºber Basis-URL")
                    return False
                
                # Pr√ºfen ob die gemeinsamen Pfad-Teile √ºbereinstimmen
                if len(base_path_parts) > 0:
                    if len(url_path_parts) == len(base_path_parts):
                        # Gleiche Ebene: ersten N-1 Teile m√ºssen √ºbereinstimmen (Nachbar-Pfade)
                        if len(base_path_parts) > 1:
                            for i in range(len(base_path_parts) - 1):
                                if url_path_parts[i] != base_path_parts[i]:
                                    logger.debug(f"URL {url} ausgeschlossen: Nachbar-Pfad stimmt nicht √ºberein bei Teil {i}")
                                    return False
                        # Letzter Teil kann unterschiedlich sein (Nachbar-Pfad)
                        logger.debug(f"URL {url} erlaubt: Nachbar-Pfad")
                        
                    elif len(url_path_parts) > len(base_path_parts):
                        # URL ist tiefer: alle Basis-Teile m√ºssen √ºbereinstimmen
                        for i, base_part in enumerate(base_path_parts):
                            if url_path_parts[i] != base_part:
                                logger.debug(f"URL {url} ausgeschlossen: Unter-Pfad stimmt nicht √ºberein bei Teil {i}")
                                return False
                        logger.debug(f"URL {url} erlaubt: Unter-Pfad")
                
                logger.debug(f"URL {url} erlaubt: hierarchische Validierung bestanden")
                return True
            
            else:
                logger.warning(f"Unbekannter Crawl-Scope: {self.crawl_scope}")
                return False
            
        except Exception as e:
            logger.error(f"Fehler beim Validieren der URL {url}: {e}")
            return False
    
    def _detect_login_page(self, soup: BeautifulSoup, url: str) -> bool:
        """Erkennt ob es sich um eine Login-Seite handelt"""
        
        # Verschiedene Indikatoren f√ºr Login-Seiten
        login_indicators = [
            # Form-basierte Erkennung
            soup.find('form', {'action': re.compile(r'login|signin|auth', re.I)}),
            soup.find('input', {'type': 'password'}),
            soup.find('input', {'name': re.compile(r'password|pass|pwd', re.I)}),
            soup.find('input', {'name': re.compile(r'username|user|email|login', re.I)}),
            
            # Button/Link-basierte Erkennung
            soup.find('button', string=re.compile(r'(log|sign)\s*in|anmelden|login', re.I)),
            soup.find('input', {'type': 'submit', 'value': re.compile(r'(log|sign)\s*in|anmelden|login', re.I)}),
            
            # URL-basierte Erkennung
            re.search(r'/(login|signin|auth|anmelden)', url, re.I),
            
            # Text-basierte Erkennung
            soup.find(string=re.compile(r'(bitte|please)\s*(log|sign)\s*in|anmeldung\s*erforderlich|authentication\s*required', re.I)),
            
            # Meta-basierte Erkennung
            soup.find('title', string=re.compile(r'login|signin|anmelden|authentication', re.I)),
            
            # Class/ID-basierte Erkennung
            soup.find(attrs={'class': re.compile(r'login|signin|auth', re.I)}),
            soup.find(attrs={'id': re.compile(r'login|signin|auth', re.I)})
        ]
        
        # Wenn mindestens 2 Indikatoren zutreffen, ist es wahrscheinlich eine Login-Seite
        indicators_found = sum(1 for indicator in login_indicators if indicator)
        
        logger.debug(f"Login-Indikatoren f√ºr {url}: {indicators_found}//{len(login_indicators)}")
        
        return indicators_found >= 2
    
    def _get_login_credentials(self, url: str) -> Optional[Dict[str, str]]:
        """Fragt Benutzer nach Login-Credentials"""
        
        # Pr√ºfen ob bereits Credentials f√ºr diese Domain vorhanden sind
        domain = urlparse(url).netloc
        if domain in self.login_credentials:
            logger.info(f"Verwende gespeicherte Credentials f√ºr {domain}")
            return self.login_credentials[domain]
        
        print(f"\nüîê Login erforderlich f√ºr {url}")
        print("=" * 60)
        
        # Benutzer fragen ob Login versucht werden soll
        attempt_login = input("M√∂chten Sie sich anmelden? (j/n, Standard: n): ").strip().lower()
        
        if attempt_login not in ['j', 'ja', 'y', 'yes']:
            print("‚è≠Ô∏è  Login √ºbersprungen - Seite wird als Login-gesch√ºtzt markiert")
            return None
        
        # Credentials abfragen
        username = input("Username/E-Mail: ").strip()
        if not username:
            print("‚ùå Kein Username angegeben - Login √ºbersprungen")
            return None
        
        password = getpass.getpass("Password: ")
        if not password:
            print("‚ùå Kein Password angegeben - Login √ºbersprungen")
            return None
        
        credentials = {
            'username': username,
            'password': password
        }
        
        # Credentials f√ºr diese Domain speichern (nur f√ºr diese Session)
        self.login_credentials[domain] = credentials
        
        print("‚úÖ Credentials gespeichert f√ºr diese Session")
        return credentials
    
    def _attempt_login(self, url: str, soup: BeautifulSoup, credentials: Dict[str, str]) -> bool:
        """Versucht Login mit den gegebenen Credentials"""
        self.stats['login_attempts'] += 1
        
        # Login-Form finden
        login_form = (
            soup.find('form', {'action': re.compile(r'login|signin|auth', re.I)}) or
            soup.find('form', lambda tag: tag and tag.find('input', {'type': 'password'}))
        )
        
        if not login_form:
            logger.warning(f"Keine Login-Form gefunden auf {url}")
            return False
        
        # Form-Action bestimmen
        action = login_form.get('action', '')
        if not action:
            action = url  # Fallback zur aktuellen URL
        else:
            action = urljoin(url, action)
        
        # Form-Daten sammeln
        form_data = {}
        
        # Alle Input-Felder finden
        for input_field in login_form.find_all('input'):
            field_name = input_field.get('name')
            field_type = input_field.get('type', 'text')
            field_value = input_field.get('value', '')
            
            if not field_name:
                continue
            
            # Username-Felder
            if (field_type.lower() in ['text', 'email'] or 
                re.search(r'username|user|email|login', field_name, re.I)):
                form_data[field_name] = credentials['username']
                logger.debug(f"Username-Feld gefunden: {field_name}")
            
            # Password-Felder
            elif field_type.lower() == 'password':
                form_data[field_name] = credentials['password']
                logger.debug(f"Password-Feld gefunden: {field_name}")
            
            # Hidden-Felder und andere Werte beibehalten
            elif field_type.lower() in ['hidden', 'submit']:
                if field_value:
                    form_data[field_name] = field_value
                    logger.debug(f"Hidden-Feld gefunden: {field_name} = {field_value}")
        
        # CSRF-Token suchen (falls vorhanden)
        csrf_token = soup.find('input', {'name': re.compile(r'csrf|token|_token', re.I)})
        if csrf_token:
            token_name = csrf_token.get('name')
            token_value = csrf_token.get('value')
            if token_name and token_value:
                form_data[token_name] = token_value
                logger.debug(f"CSRF-Token gefunden: {token_name}")
        
        logger.info(f"Versuche Login auf {action} mit {len(form_data)} Feldern")
        
        # POST-Request f√ºr Login
        response = self.session.post(
            action,
            data=form_data,
            timeout=15,
            allow_redirects=True
        )
        
        # Login-Erfolg pr√ºfen
        success_indicators = [
            response.status_code == 200,
            'dashboard' in response.url.lower(),
            'profile' in response.url.lower(),
            'welcome' in response.text.lower(),
            'logout' in response.text.lower(),
            not self._detect_login_page(BeautifulSoup(response.content, 'html.parser'), response.url)
        ]
        
        login_success = sum(success_indicators) >= 2
        
        if login_success:
            logger.info(f"‚úÖ Login erfolgreich f√ºr {urlparse(url).netloc}")
            self.stats['login_successes'] += 1
            print(f"‚úÖ Login erfolgreich f√ºr {urlparse(url).netloc}")
            return True
        else:
            logger.warning(f"‚ùå Login fehlgeschlagen f√ºr {urlparse(url).netloc}")
            print(f"‚ùå Login fehlgeschlagen f√ºr {urlparse(url).netloc}")
            return False
                
    def _extract_pagination_links(self, soup: BeautifulSoup, base_url: str) -> List[str]:
        """Extrahiert Pagination-Links (Next, N√§chste Seite, etc.)"""
        pagination_urls = []
        
        # Verschiedene Pagination-Begriffe in mehreren Sprachen
        pagination_patterns = [
            # Deutsch
            r'n√§chste\s*(seite|page)?', r'weiter', r'vorw√§rts', r'folgende',
            # Englisch
            r'next\s*(page|step)?', r'forward', r'continue', r'more',
            # Franz√∂sisch
            r'suivant', r'prochain', r'continuer',
            # Spanisch
            r'siguiente', r'pr√≥ximo', r'continuar',
            # Italienisch
            r'successivo', r'avanti', r'continua',
            # Allgemein
            r'‚Üí', r'‚ñ∫', r'¬ª', r'>>',
            # Numerische Pagination
            r'page\s*\d+', r'seite\s*\d+', r'\d+\s*/'
        ]
        
        # Kompilierte Regex-Pattern f√ºr bessere Performance
        compiled_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in pagination_patterns]
        
        # Suche nach Pagination-Links
        for link in soup.find_all('a', href=True):
            href = link.get('href')
            if not href:
                continue
            
            # Link-Text und umgebende Elemente analysieren
            link_text = link.get_text(strip=True).lower()
            
            # Auch title, aria-label und andere Attribute pr√ºfen
            title = (link.get('title') or '').lower()
            aria_label = (link.get('aria-label') or '').lower()
            class_attr = ' '.join(link.get('class', [])).lower()
            
            # Alle relevanten Texte kombinieren
            all_text = f"{link_text} {title} {aria_label} {class_attr}"
            
            # Pr√ºfen ob einer der Pagination-Pattern zutrifft
            for pattern in compiled_patterns:
                if pattern.search(all_text):
                    absolute_url = urljoin(base_url, href)
                    absolute_url = absolute_url.split('#')[0]  # Anker entfernen
                    
                    if self._is_valid_url(absolute_url):
                        pagination_urls.append(absolute_url)
                        logger.info(f"Pagination-Link gefunden: {link_text} -> {absolute_url}")
                    break
        
        # Auch Buttons und andere Elemente pr√ºfen
        for button in soup.find_all(['button', 'input'], type=['button', 'submit']):
            onclick = button.get('onclick') or ''
            value = button.get('value') or ''
            button_text = button.get_text(strip=True).lower()
            
            all_text = f"{button_text} {value} {onclick}".lower()
            
            for pattern in compiled_patterns:
                if pattern.search(all_text):
                    # JavaScript-Links extrahieren (falls vorhanden)
                    url_match = re.search(r'location\.href\s*=\s*["\']([^"\']+)["\']', onclick)
                    if url_match:
                        absolute_url = urljoin(base_url, url_match.group(1))
                        if self._is_valid_url(absolute_url):
                            pagination_urls.append(absolute_url)
                            logger.info(f"Button-Pagination gefunden: {button_text} -> {absolute_url}")
                    break
        
        return list(set(pagination_urls))  # Duplikate entfernen
        """Extrahiert Pagination-Links (Next, N√§chste Seite, etc.)"""
        pagination_urls = []
        
        # Verschiedene Pagination-Begriffe in mehreren Sprachen
        pagination_patterns = [
            # Deutsch
            r'n√§chste\s*(seite|page)?', r'weiter', r'vorw√§rts', r'folgende',
            # Englisch
            r'next\s*(page|step)?', r'forward', r'continue', r'more',
            # Franz√∂sisch
            r'suivant', r'prochain', r'continuer',
            # Spanisch
            r'siguiente', r'pr√≥ximo', r'continuar',
            # Italienisch
            r'successivo', r'avanti', r'continua',
            # Allgemein
            r'‚Üí', r'‚ñ∫', r'¬ª', r'>>',
            # Numerische Pagination
            r'page\s*\d+', r'seite\s*\d+', r'\d+\s*/'
        ]
        
        # Kompilierte Regex-Pattern f√ºr bessere Performance
        compiled_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in pagination_patterns]
        
        # Suche nach Pagination-Links
        for link in soup.find_all('a', href=True):
            href = link.get('href')
            if not href:
                continue
            
            # Link-Text und umgebende Elemente analysieren
            link_text = link.get_text(strip=True).lower()
            
            # Auch title, aria-label und andere Attribute pr√ºfen
            title = (link.get('title') or '').lower()
            aria_label = (link.get('aria-label') or '').lower()
            class_attr = ' '.join(link.get('class', [])).lower()
            
            # Alle relevanten Texte kombinieren
            all_text = f"{link_text} {title} {aria_label} {class_attr}"
            
            # Pr√ºfen ob einer der Pagination-Pattern zutrifft
            for pattern in compiled_patterns:
                if pattern.search(all_text):
                    absolute_url = urljoin(base_url, href)
                    absolute_url = absolute_url.split('#')[0]  # Anker entfernen
                    
                    if self._is_valid_url(absolute_url):
                        pagination_urls.append(absolute_url)
                        logger.info(f"Pagination-Link gefunden: {link_text} -> {absolute_url}")
                    break
        
        # Auch Buttons und andere Elemente pr√ºfen
        for button in soup.find_all(['button', 'input'], type=['button', 'submit']):
            onclick = button.get('onclick') or ''
            value = button.get('value') or ''
            button_text = button.get_text(strip=True).lower()
            
            all_text = f"{button_text} {value} {onclick}".lower()
            
            for pattern in compiled_patterns:
                if pattern.search(all_text):
                    # JavaScript-Links extrahieren (falls vorhanden)
                    url_match = re.search(r'location\.href\s*=\s*["\']([^"\']+)["\']', onclick)
                    if url_match:
                        absolute_url = urljoin(base_url, url_match.group(1))
                        if self._is_valid_url(absolute_url):
                            pagination_urls.append(absolute_url)
                            logger.info(f"Button-Pagination gefunden: {button_text} -> {absolute_url}")
                    break
        
        return list(set(pagination_urls))  # Duplikate entfernen
    def _extract_links(self, soup: BeautifulSoup, base_url: str) -> List[Tuple[str, str]]:
        """Extrahiert alle Links aus einer Seite"""
        links = []
        new_urls = []
        
        # Erst normale Links extrahieren
        for link in soup.find_all('a', href=True):
            href = link.get('href')
            if not href:
                continue
                
            # Relative URLs zu absoluten URLs konvertieren
            absolute_url = urljoin(base_url, href)
            
            # Ankerlinks entfernen
            absolute_url = absolute_url.split('#')[0]
            
            # Link-Text extrahieren
            link_text = link.get_text(strip=True) or href
            
            links.append((link_text, absolute_url))
            
            # URL-Validierung und Queue-Management
            if self._is_valid_url(absolute_url):
                if absolute_url not in self.visited_urls and absolute_url not in self.queue:
                    new_urls.append(absolute_url)
                    logger.info(f"Neue g√ºltige URL gefunden: {absolute_url}")
            else:
                # Statistik f√ºr ung√ºltige URLs
                parsed = urlparse(absolute_url)
                if parsed.netloc != self.base_domain:
                    self.stats['skipped_external'] += 1
                    logger.debug(f"Externe URL √ºbersprungen: {absolute_url}")
                else:
                    self.stats['skipped_invalid'] += 1
                    logger.debug(f"Ung√ºltige URL √ºbersprungen: {absolute_url}")
        
        # Pagination-Links extrahieren (h√∂here Priorit√§t)
        pagination_urls = self._extract_pagination_links(soup, base_url)
        self.stats['pagination_links_found'] += len(pagination_urls)
        
        # Normale URLs z√§hlen
        self.stats['total_links_found'] += len(new_urls)
        
        # Pagination-URLs zur Queue hinzuf√ºgen (mit h√∂herer Priorit√§t)
        for page_url in pagination_urls:
            if page_url not in self.visited_urls and page_url not in self.queue:
                # Pagination-Links werden am Anfang der Queue eingef√ºgt (h√∂here Priorit√§t)
                self.queue.insert(0, page_url)
                logger.info(f"Pagination-URL zur Queue hinzugef√ºgt: {page_url}")
        
        # Normale URLs am Ende der Queue hinzuf√ºgen
        for url in new_urls:
            if url not in pagination_urls:  # Vermeiden von Duplikaten
                self.queue.append(url)
                logger.info(f"Normale URL zur Queue hinzugef√ºgt: {url}")
        
        logger.info(f"Link-Extraktion abgeschlossen: {len(links)} Links gefunden, {len(new_urls)} neue URLs, {len(pagination_urls)} Pagination-URLs")
        return links
    
    def _download_image(self, img_url: str, img_name: str) -> Optional[str]:
        """L√§dt ein Bild herunter und speichert es"""
        try:
            response = self.session.get(img_url, timeout=10)
            response.raise_for_status()
            
            img_path = self.images_dir / img_name
            
            with open(img_path, 'wb') as f:
                f.write(response.content)
            
            # Bildgr√∂√üe f√ºr Web/PDF optimieren
            try:
                with Image.open(img_path) as img:
                    # Bild f√ºr Web-Anzeige optimieren (max 1200px breit)
                    if img.width > 1200:
                        ratio = 1200 / img.width
                        new_height = int(img.height * ratio)
                        img = img.resize((1200, new_height), Image.Resampling.LANCZOS)
                        img.save(img_path, optimize=True, quality=85)
            except Exception as e:
                logger.debug(f"Bildoptimierung fehlgeschlagen f√ºr {img_name}: {e}")
                pass  # Bild trotzdem verwenden
            
            self.stats['images_downloaded'] += 1
            return str(img_path.absolute())
            
        except Exception as e:
            logger.error(f"Fehler beim Herunterladen von {img_url}: {e}")
            return None
    
    def _extract_images_and_embed(self, soup: BeautifulSoup, base_url: str) -> List[Dict[str, str]]:
        """Extrahiert Bilder und embeddet sie direkt in das HTML als Base64"""
        images = []
        
        for idx, img in enumerate(soup.find_all('img', src=True)):
            src = img.get('src')
            if not src:
                continue
            
            # Relative URLs zu absoluten URLs konvertieren
            img_url = urljoin(base_url, src)
            
            # Dateiname generieren
            img_name = os.path.basename(urlparse(img_url).path)
            if not img_name or '.' not in img_name:
                img_name = f"image_{idx}.jpg"
            
            # Dateiname bereinigen
            img_name = re.sub(r'[<>:"/\\|?*]', '_', img_name)
            
            # Bild herunterladen
            img_path = self._download_image(img_url, img_name)
            if img_path:
                try:
                    # Bild als Base64 einbetten
                    with open(img_path, 'rb') as f:
                        img_data = f.read()
                    
                    # MIME-Type bestimmen
                    if img_path.lower().endswith(('.png', '.PNG')):
                        mime_type = 'image/png'
                    elif img_path.lower().endswith(('.jpg', '.jpeg', '.JPG', '.JPEG')):
                        mime_type = 'image/jpeg'
                    elif img_path.lower().endswith(('.gif', '.GIF')):
                        mime_type = 'image/gif'
                    elif img_path.lower().endswith(('.svg', '.SVG')):
                        mime_type = 'image/svg+xml'
                    else:
                        mime_type = 'image/jpeg'  # Fallback
                    
                    # Base64-Kodierung
                    img_base64 = base64.b64encode(img_data).decode('utf-8')
                    data_uri = f"data:{mime_type};base64,{img_base64}"
                    
                    # Img-Tag im HTML aktualisieren
                    img['src'] = data_uri
                    
                    # Responsive Bildgr√∂√üe f√ºr PDF setzen
                    img['style'] = img.get('style', '') + ' max-width: 100%; height: auto;'
                    
                    # Bild-Metadaten sammeln
                    img_data_info = {
                        'path': img_path,
                        'url': img_url,
                        'alt': img.get('alt', ''),
                        'title': img.get('title', ''),
                        'position': idx,
                        'embedded': True
                    }
                    images.append(img_data_info)
                    
                    logger.debug(f"Bild als Base64 eingebettet: {img_name}")
                    
                except Exception as e:
                    logger.error(f"Fehler beim Einbetten von Bild {img_path}: {e}")
                    # Fallback: urspr√ºngliche URL beibehalten
                    img['src'] = img_url
        
        return images
    
    def _clean_text(self, text: str) -> str:
        """Bereinigt Text von √ºberfl√ºssigen Whitespaces"""
        # Mehrfache Leerzeichen und Zeilenumbr√ºche reduzieren
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'\n\s*\n', '\n\n', text)
        return text.strip()
    
    def _create_text_content(self, page: WebPage) -> str:
        """Erstellt den Textinhalt mit Links und Bildpfaden"""
        content = f"Titel: {page.title}\n"
        content += f"URL: {page.url}\n"
        content += f"Zeitpunkt: {page.timestamp}\n"
        content += "=" * 50 + "\n\n"
        
        # Hauptinhalt mit eingef√ºgten Bildern
        main_content = page.content
        
        # Bilder in den Text einf√ºgen
        for img_data in page.images:
            placeholder = f"[IMAGE_{img_data['position']}]"
            if placeholder in main_content:
                img_info = f"\n\n[BILD: {img_data['path']}"
                if img_data['alt']:
                    img_info += f" | Alt: {img_data['alt']}"
                if img_data['title']:
                    img_info += f" | Titel: {img_data['title']}"
                img_info += f" | URL: {img_data['url']}]\n\n"
                main_content = main_content.replace(placeholder, img_info)
        
        content += main_content + "\n\n"
        
        # Links hinzuf√ºgen
        if page.links:
            content += "LINKS:\n"
            content += "-" * 10 + "\n"
            for link_text, link_url in page.links:
                content += f"[{link_text}|{link_url}]\n"
            content += "\n"
        
        # Alle Bilder am Ende auflisten
        if page.images:
            content += "BILDER-VERZEICHNIS:\n"
            content += "-" * 20 + "\n"
            for img_data in page.images:
                content += f"Datei: {img_data['path']}\n"
                content += f"URL: {img_data['url']}\n"
                if img_data['alt']:
                    content += f"Alt-Text: {img_data['alt']}\n"
                if img_data['title']:
                    content += f"Titel: {img_data['title']}\n"
                content += "\n"
        
        # Quelle hinzuf√ºgen
        content += f"Quelle: {page.url}\n"
        
        return content
    
    def _create_pdf_html(self, page: WebPage, original_html: str) -> None:
        """Erstellt ein PDF aus HTML mit vollst√§ndiger Formatierung"""
        try:
            # HTML f√ºr PDF aufbereiten
            soup = BeautifulSoup(original_html, 'html.parser')
            
            # √úberfl√ºssige Elemente f√ºr PDF entfernen
            for element in soup(['script', 'style', 'nav', 'footer', 'header', 'aside', 'iframe']):
                element.decompose()
            
            # Content-Bereich extrahieren
            content_element = (
                soup.find('main') or 
                soup.find('article') or 
                soup.find('div', class_=re.compile(r'content|main|article', re.I)) or
                soup.find('body')
            )
            
            if not content_element:
                content_element = soup
            
            # Bilder einbetten
            images = self._extract_images_and_embed(content_element, page.url)
            
            # CSS f√ºr PDF-Styling
            pdf_css = """
            @page {
                size: A4;
                margin: 2cm 2cm 3cm 2cm;
                
                @bottom-left {
                    content: "Quelle: """ + page.url + """";
                    font-size: 10px;
                    font-family: Arial, sans-serif;
                    color: #666;
                }
                
                @bottom-right {
                    content: "Seite " counter(page) " von " counter(pages);
                    font-size: 10px;
                    font-family: Arial, sans-serif;
                    color: #666;
                }
            }
            
            body {
                font-family: Arial, Helvetica, sans-serif;
                font-size: 12px;
                line-height: 1.6;
                color: #333;
                margin: 0;
                padding: 0;
            }
            
            h1, h2, h3, h4, h5, h6 {
                color: #2c3e50;
                margin: 1.5em 0 0.5em 0;
                page-break-after: avoid;
            }
            
            h1 {
                font-size: 24px;
                border-bottom: 2px solid #3498db;
                padding-bottom: 10px;
            }
            
            h2 {
                font-size: 20px;
                border-bottom: 1px solid #bdc3c7;
                padding-bottom: 5px;
            }
            
            h3 {
                font-size: 18px;
                color: #34495e;
            }
            
            h4 {
                font-size: 16px;
                color: #34495e;
            }
            
            p {
                margin: 0.8em 0;
                text-align: justify;
            }
            
            img {
                max-width: 100%;
                height: auto;
                display: block;
                margin: 1em auto;
                border: 1px solid #ddd;
                border-radius: 4px;
                padding: 5px;
                background-color: #f9f9f9;
            }
            
            code {
                background-color: #f4f4f4;
                padding: 2px 4px;
                border-radius: 3px;
                font-family: "Courier New", monospace;
                font-size: 11px;
            }
            
            pre {
                background-color: #f4f4f4;
                padding: 15px;
                border-radius: 5px;
                border-left: 4px solid #3498db;
                overflow-x: auto;
                font-family: "Courier New", monospace;
                font-size: 11px;
                line-height: 1.4;
            }
            
            blockquote {
                border-left: 4px solid #3498db;
                padding-left: 20px;
                margin: 1em 0;
                font-style: italic;
                color: #555;
            }
            
            ul, ol {
                margin: 1em 0;
                padding-left: 2em;
            }
            
            li {
                margin: 0.3em 0;
            }
            
            table {
                border-collapse: collapse;
                width: 100%;
                margin: 1em 0;
                font-size: 11px;
            }
            
            th, td {
                border: 1px solid #ddd;
                padding: 8px;
                text-align: left;
            }
            
            th {
                background-color: #f2f2f2;
                font-weight: bold;
            }
            
            tr:nth-child(even) {
                background-color: #f9f9f9;
            }
            
            a {
                color: #3498db;
                text-decoration: none;
            }
            
            a:hover {
                text-decoration: underline;
            }
            
            .highlight {
                background-color: #fff3cd;
                padding: 2px 4px;
                border-radius: 3px;
            }
            
            .text-center {
                text-align: center;
            }
            
            .text-right {
                text-align: right;
            }
            
            .page-break {
                page-break-before: always;
            }
            
            .no-break {
                page-break-inside: avoid;
            }
            
            /* Responsive Tabellen f√ºr PDF */
            @media print {
                table {
                    page-break-inside: auto;
                }
                
                tr {
                    page-break-inside: avoid;
                    page-break-after: auto;
                }
            }
            """
            
            # Vollst√§ndige HTML-Struktur erstellen
            html_content = f"""
            <!DOCTYPE html>
            <html lang="de">
            <head>
                <meta charset="UTF-8">
                <meta name="viewport" content="width=device-width, initial-scale=1.0">
                <title>{page.title}</title>
                <style>{pdf_css}</style>
            </head>
            <body>
                <div class="header">
                    <h1>{page.title}</h1>
                    <p><strong>URL:</strong> {page.url}</p>
                    <p><strong>Archiviert am:</strong> {page.timestamp}</p>
                    <hr>
                </div>
                
                <div class="content">
                    {content_element}
                </div>
            </body>
            </html>
            """
            
            # PDF mit WeasyPrint erstellen
            html_doc = HTML(string=html_content)
            html_doc.write_pdf(page.pdf_path)
            
            # Bilder-Info aktualisieren
            page.images = images
            
            logger.info(f"PDF mit HTML-Formatting erstellt: {page.pdf_path} ({len(images)} Bilder eingebettet)")
            
        except Exception as e:
            logger.error(f"Fehler beim Erstellen des HTML-PDFs f√ºr {page.url}: {e}")
            # Fallback: Einfaches Text-PDF erstellen
            try:
                self._create_simple_pdf_fallback(page)
            except Exception as fallback_error:
                logger.error(f"Auch Fallback-PDF fehlgeschlagen: {fallback_error}")
    
    def _create_simple_pdf_fallback(self, page: WebPage) -> None:
        """Erstellt ein einfaches Text-PDF als Fallback"""
        try:
            # Einfaches HTML f√ºr Fallback
            simple_html = f"""
            <!DOCTYPE html>
            <html>
            <head>
                <meta charset="UTF-8">
                <title>{page.title}</title>
                <style>
                    @page {{
                        size: A4;
                        margin: 2cm;
                        @bottom-left {{
                            content: "Quelle: {page.url}";
                            font-size: 10px;
                        }}
                        @bottom-right {{
                            content: "Seite " counter(page) " von " counter(pages);
                            font-size: 10px;
                        }}
                    }}
                    body {{
                        font-family: Arial, sans-serif;
                        font-size: 12px;
                        line-height: 1.6;
                    }}
                    h1 {{
                        color: #333;
                        border-bottom: 2px solid #ccc;
                        padding-bottom: 10px;
                    }}
                    p {{
                        margin: 1em 0;
                        text-align: justify;
                    }}
                </style>
            </head>
            <body>
                <h1>{page.title}</h1>
                <p><strong>URL:</strong> {page.url}</p>
                <p><strong>Archiviert am:</strong> {page.timestamp}</p>
                <hr>
                <div>
                    {'<p>' + page.content.replace('\n\n', '</p><p>') + '</p>'}
                </div>
            </body>
            </html>
            """
            
            html_doc = HTML(string=simple_html)
            html_doc.write_pdf(page.pdf_path)
            
            logger.info(f"Fallback-PDF erstellt: {page.pdf_path}")
            
        except Exception as e:
            logger.error(f"Fallback-PDF fehlgeschlagen: {e}")
    
    def _process_url(self, url: str) -> Optional[WebPage]:
        """Verarbeitet eine einzelne URL mit Login-Support"""
        try:
            logger.info(f"Verarbeite URL: {url}")
            
            # Ersten Request machen
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            
            # Login-Detection
            soup = BeautifulSoup(response.content, 'html.parser')
            
            if self._detect_login_page(soup, url):
                self.stats['login_pages_found'] += 1
                logger.info(f"üîê Login-Seite erkannt: {url}")
                
                # Pr√ºfen ob bereits Login versucht wurde f√ºr diese Domain
                domain = urlparse(url).netloc
                if domain not in self.login_attempted:
                    self.login_attempted.add(domain)
                    
                    # Credentials abfragen
                    credentials = self._get_login_credentials(url)
                    
                    if credentials:
                        # Login versuchen
                        if self._attempt_login(url, soup, credentials):
                            # Nach erfolgreichem Login erneut versuchen
                            response = self.session.get(url, timeout=15)
                            response.raise_for_status()
                            soup = BeautifulSoup(response.content, 'html.parser')
                        else:
                            # Login fehlgeschlagen - erstelle Login-gesch√ºtzte Seite
                            return self._create_login_protected_page(url)
                    else:
                        # Keine Credentials - erstelle Login-gesch√ºtzte Seite
                        return self._create_login_protected_page(url)
                else:
                    # Login bereits versucht - pr√ºfen ob Session noch g√ºltig
                    if domain in self.login_credentials:
                        logger.info(f"Verwende bestehende Session f√ºr {domain}")
                    else:
                        # Session ung√ºltig oder Login nicht versucht
                        return self._create_login_protected_page(url)
            
            # Urspr√ºngliches HTML f√ºr PDF-Erstellung speichern
            original_html = response.text
            
            # Backup der urspr√ºnglichen Seite f√ºr Bildextraktion
            original_soup = BeautifulSoup(response.content, 'html.parser')
            
            # Arbeits-Kopie f√ºr Content-Extraktion
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Titel extrahieren
            title = soup.find('title')
            title = title.get_text(strip=True) if title else "Ohne Titel"
            
            # F√ºr Textdatei: vereinfachte Bildextraktion
            text_images = []
            for idx, img in enumerate(soup.find_all('img', src=True)):
                src = img.get('src')
                if src:
                    img_url = urljoin(url, src)
                    img_name = os.path.basename(urlparse(img_url).path) or f"image_{idx}.jpg"
                    img_name = re.sub(r'[<>:"/\\|?*]', '_', img_name)
                    
                    img_path = self._download_image(img_url, img_name)
                    if img_path:
                        img_data = {
                            'path': img_path,
                            'url': img_url,
                            'alt': img.get('alt', ''),
                            'title': img.get('title', ''),
                            'position': idx
                        }
                        text_images.append(img_data)
            
            # Content f√ºr Textdatei extrahieren
            # √úberfl√ºssige Elemente entfernen
            for element in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):
                element.decompose()
            
            # Content-Bereich finden
            content_element = (
                soup.find('main') or 
                soup.find('article') or 
                soup.find('div', class_=re.compile(r'content|main|article', re.I)) or
                soup.find('body')
            )
            
            if content_element:
                content = content_element.get_text(separator='\n', strip=True)
            else:
                content = soup.get_text(separator='\n', strip=True)
            
            content = self._clean_text(content)
            
            # Links extrahieren (mit Original-Soup)
            links = self._extract_links(original_soup, url)
            
            # Dateinamen erstellen
            safe_title = re.sub(r'[<>:"/\\|?*]', '_', title)[:100]
            file_name = f"{len(self.pages) + 1:03d}_{safe_title}"
            
            # Textdatei im /texts Unterordner speichern
            text_path = self.texts_dir / f"{file_name}.txt"
            pdf_path = self.output_dir / f"{file_name}.pdf"
            
            # WebPage-Objekt erstellen
            page = WebPage(
                url=url,
                title=title,
                content=content,
                images=text_images,  # Vorl√§ufig f√ºr Textdatei
                links=links,
                file_path=str(text_path),
                pdf_path=str(pdf_path),
                timestamp=time.strftime("%Y-%m-%d %H:%M:%S")
            )
            
            # Textdatei erstellen
            text_content = self._create_text_content(page)
            with open(text_path, 'w', encoding='utf-8') as f:
                f.write(text_content)
            
            # PDF erstellen (einzeln oder f√ºr sp√§teren Merge)
            self._create_pdf_html(page, original_html)
            
            # PDF-Pfad f√ºr eventuelle Fusion speichern
            if self.single_pdf:
                self.pdf_paths.append(str(pdf_path))
            
            self.stats['processed'] += 1
            logger.info(f"Seite erfolgreich verarbeitet: {title} ({len(text_images)} Bilder)")
            
            return page
            
        except Exception as e:
            logger.error(f"Fehler beim Verarbeiten von {url}: {e}")
            self.stats['errors'] += 1
            return None
    
    def _create_login_protected_page(self, url: str) -> WebPage:
        """Erstellt eine Placeholder-Seite f√ºr Login-gesch√ºtzte Inhalte"""
        
        title = "Login-gesch√ºtzte Seite"
        content = "Diese Seite ist nur √ºber einen Login erreichbar und konnte nicht automatisch verarbeitet werden."
        
        # Dateinamen erstellen
        safe_title = re.sub(r'[<>:"/\\|?*]', '_', title)[:100]
        file_name = f"{len(self.pages) + 1:03d}_{safe_title}_{urlparse(url).path.split('/')[-1]}"
        
        text_path = self.texts_dir / f"{file_name}.txt"
        pdf_path = self.output_dir / f"{file_name}.pdf"
        
        # WebPage-Objekt erstellen
        page = WebPage(
            url=url,
            title=title,
            content=content,
            images=[],
            links=[],
            file_path=str(text_path),
            pdf_path=str(pdf_path),
            timestamp=time.strftime("%Y-%m-%d %H:%M:%S")
        )
        
        # Textdatei erstellen
        text_content = self._create_text_content(page)
        with open(text_path, 'w', encoding='utf-8') as f:
            f.write(text_content)
        
        # Einfaches PDF f√ºr Login-gesch√ºtzte Seite erstellen
        self._create_simple_pdf_fallback(page)
        
        if self.single_pdf:
            self.pdf_paths.append(str(pdf_path))
        
        logger.info(f"Login-gesch√ºtzte Seite erstellt: {title}")
        return page
    
    def _merge_pdfs(self) -> None:
        """Fusioniert alle einzelnen PDFs zu einem gro√üen PDF"""
        if not self.pdf_paths:
            logger.warning("Keine PDFs zum Fusionieren gefunden")
            return
        
        try:
            merger = PdfMerger()
            
            # Alle PDFs in der richtigen Reihenfolge hinzuf√ºgen
            for pdf_path in self.pdf_paths:
                if os.path.exists(pdf_path):
                    merger.append(pdf_path)
                    logger.debug(f"PDF zur Fusion hinzugef√ºgt: {pdf_path}")
                else:
                    logger.warning(f"PDF nicht gefunden: {pdf_path}")
            
            # Fusioniertes PDF speichern
            merged_pdf_path = self.output_dir / "Website_Komplett.pdf"
            merger.write(str(merged_pdf_path))
            merger.close()
            
            logger.info(f"‚úÖ PDFs erfolgreich fusioniert: {merged_pdf_path}")
            print(f"üìñ Einzelne PDFs wurden zu einer Datei zusammengef√ºgt: {merged_pdf_path}")
            
            # Einzelne PDFs optional l√∂schen (User fragen)
            delete_individual = input("\nüóëÔ∏è  Einzelne PDF-Dateien l√∂schen? (j/n, Standard: n): ").strip().lower()
            if delete_individual in ['j', 'ja', 'y', 'yes']:
                deleted_count = 0
                for pdf_path in self.pdf_paths:
                    try:
                        os.remove(pdf_path)
                        deleted_count += 1
                    except Exception as e:
                        logger.error(f"Fehler beim L√∂schen von {pdf_path}: {e}")
                
                logger.info(f"üóëÔ∏è  {deleted_count} einzelne PDF-Dateien gel√∂scht")
                print(f"üóëÔ∏è  {deleted_count} einzelne PDF-Dateien gel√∂scht")
        
        except Exception as e:
            logger.error(f"Fehler beim Fusionieren der PDFs: {e}")
            print(f"‚ùå Fehler beim Fusionieren der PDFs: {e}")
    
    def _create_url_collection_json(self) -> None:
        """Erstellt url-collection.json (ehemals summary.json)"""
        # Seiten nach URL-Pfad gruppieren
        grouped_pages = defaultdict(list)
        
        for page in self.pages:
            parsed_url = urlparse(page.url)
            path_parts = parsed_url.path.split('/')
            
            # Gruppierung nach Pfad-Tiefe
            if len(path_parts) <= 2:
                group_key = "/"
            else:
                group_key = '/'.join(path_parts[:3])
            
            grouped_pages[group_key].append(page)
        
        # Innerhalb jeder Gruppe nach URL sortieren
        for group in grouped_pages.values():
            group.sort(key=lambda x: x.url)
        
        # JSON-Struktur erstellen
        json_data = {
            "base_url": self.base_url,
            "crawl_timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "single_pdf_created": self.single_pdf,
            "statistics": self.stats,
            "groups": {}
        }
        
        for group_key, pages in grouped_pages.items():
            json_data["groups"][group_key] = []
            for page in pages:
                # Bilder-Informationen f√ºr JSON aufbereiten
                image_info = []
                for img_data in page.images:
                    image_info.append({
                        "path": img_data["path"],
                        "url": img_data["url"],
                        "alt": img_data.get("alt", ""),
                        "title": img_data.get("title", ""),
                        "position": img_data.get("position", 0)
                    })
                
                page_data = {
                    "url": page.url,
                    "title": page.title,
                    "content": page.content[:500] + "..." if len(page.content) > 500 else page.content,
                    "images": image_info,
                    "links": page.links,
                    "file_path": page.file_path,
                    "pdf_path": page.pdf_path,
                    "timestamp": page.timestamp
                }
                json_data["groups"][group_key].append(page_data)
        
        json_path = self.output_dir / "url-collection.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"URL-Collection JSON erstellt: {json_path}")
    
    def _create_summary_json(self) -> None:
        """Erstellt das neue summary.json in Sitemap-Reihenfolge"""
        
        # URLs in derselben Reihenfolge wie in der Sitemap sortieren
        sorted_pages = sorted(self.pages, key=lambda x: x.url)
        
        # JSON-Struktur f√ºr Summary erstellen
        summary_data = []
        
        for page in sorted_pages:
            # Bildpfade als Liste extrahieren
            image_paths = [img_data["path"] for img_data in page.images]
            
            page_summary = {
                "title": page.title,
                "url": page.url,
                "textcontent": page.content,
                "image_paths": image_paths,
                "date": time.strftime("%Y-%m-%d")
            }
            
            summary_data.append(page_summary)
        
        # Summary JSON speichern
        summary_path = self.output_dir / "summary.json"
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"Summary JSON erstellt: {summary_path}")
        
        # Zus√§tzliche Bild-Statistiken
        total_images = sum(len(page.images) for page in self.pages)
        successful_images = sum(1 for page in self.pages for img in page.images if os.path.exists(img['path']))
        
        logger.info(f"Bilder-Statistiken: {successful_images}/{total_images} erfolgreich heruntergeladen")
    
    def _create_sitemap(self) -> None:
        """Erstellt eine Sitemap-Datei"""
        sitemap_content = f"SITEMAP f√ºr {self.base_url}\n"
        sitemap_content += "=" * 50 + "\n\n"
        
        # URLs hierarchisch sortieren
        urls = [page.url for page in self.pages]
        urls.sort()
        
        # Hierarchische Struktur aufbauen
        url_tree = {}
        for url in urls:
            parsed = urlparse(url)
            path_parts = [part for part in parsed.path.split('/') if part]
            
            current = url_tree
            for part in path_parts:
                if part not in current:
                    current[part] = {}
                current = current[part]
            current['__url__'] = url
        
        def build_tree_string(tree, indent=0):
            result = ""
            for key, value in sorted(tree.items()):
                if key == '__url__':
                    continue
                spaces = "  " * indent
                result += f"{spaces}{key}/\n"
                if isinstance(value, dict):
                    if '__url__' in value:
                        result += f"{spaces}  ‚Üí {value['__url__']}\n"
                    result += build_tree_string(value, indent + 1)
            return result
        
        sitemap_content += build_tree_string(url_tree)
        sitemap_content += "\n\nAlle URLs:\n"
        sitemap_content += "-" * 20 + "\n"
        for page in self.pages:
            sitemap_content += f"{page.url}\n"
        
        sitemap_path = self.output_dir / "sitemap.txt"
        with open(sitemap_path, 'w', encoding='utf-8') as f:
            f.write(sitemap_content)
        
        logger.info(f"Sitemap erstellt: {sitemap_path}")
    
    def crawl(self) -> None:
        """Hauptmethode zum Crawlen der Website"""
        logger.info(f"Starte Crawling von: {self.base_url}")
        logger.info(f"Maximale URLs: {self.max_urls}")
        logger.info(f"PDF-Modus: {'Einzelnes PDF' if self.single_pdf else 'Separate PDFs'}")
        logger.info(f"Crawl-Scope: {self.crawl_scope.value}")
        
        while self.queue and len(self.visited_urls) < self.max_urls:
            url = self.queue.pop(0)
            
            if url in self.visited_urls:
                continue
            
            self.visited_urls.add(url)
            
            # URL verarbeiten
            page = self._process_url(url)
            if page:
                self.pages.append(page)
            
            # Kurze Pause zwischen Requests
            time.sleep(1)
            
            # Fortschritt anzeigen mit detaillierten Informationen
            progress = len(self.visited_urls)
            queue_size = len(self.queue)
            
            print(f"\rüîÑ Verarbeitet: {progress}/{self.max_urls} URLs | "
                  f"Warteschlange: {queue_size} | "
                  f"Pagination: {self.stats['pagination_links_found']} | "
                  f"Bilder: {self.stats['images_downloaded']} | "
                  f"Login: {self.stats['login_pages_found']} | "
                  f"Scope: {self.crawl_scope.value} | "
                  f"Fehler: {self.stats['errors']}", end="", flush=True)
            
            # Bei bestimmten Meilensteinen zus√§tzliche Info ausgeben
            if progress % 10 == 0 and progress > 0:
                print(f"\nüìä Zwischenbericht nach {progress} URLs:")
                print(f"   ‚úÖ Erfolgreich: {self.stats['processed']}")
                print(f"   ‚ùå Fehler: {self.stats['errors']}")
                print(f"   üñºÔ∏è  Bilder: {self.stats['images_downloaded']}")
                print(f"   üìÑ Pagination: {self.stats['pagination_links_found']}")
                print(f"   üîó Links: {self.stats['total_links_found']}")
                print(f"   üîê Login-Seiten: {self.stats['login_pages_found']}")
                print(f"   üéØ Scope: {self.crawl_scope.value}")
                
                # Aktuelle URL anzeigen
                if self.pages:
                    last_page = self.pages[-1]
                    print(f"   üåê Letzte URL: {last_page.url}")
                    print(f"   üìù Titel: {last_page.title[:50]}...")
                    
                print()  # Neue Zeile f√ºr bessere Lesbarkeit
        
        print("\n")  # Abschlie√üende neue Zeile
        
        # PDF-Fusion falls gew√ºnscht
        if self.single_pdf and self.pdf_paths:
            logger.info("üìñ Starte PDF-Fusion...")
            self._merge_pdfs()
        
        # Abschlie√üende Dateien erstellen
        self._create_url_collection_json()
        self._create_summary_json()
        self._create_sitemap()
        
        # Statistiken ausgeben
        logger.info("=" * 60)
        logger.info("CRAWLING ABGESCHLOSSEN - STATISTIKEN")
        logger.info("=" * 60)
        logger.info(f"üìä Verarbeitete URLs: {self.stats['processed']}")
        logger.info(f"‚ùå Fehler: {self.stats['errors']}")
        logger.info(f"üñºÔ∏è  Heruntergeladene Bilder: {self.stats['images_downloaded']}")
        logger.info(f"üìÑ Pagination-Links gefunden: {self.stats['pagination_links_found']}")
        logger.info(f"üîó Gesamte Links gefunden: {self.stats['total_links_found']}")
        logger.info(f"üåê Externe URLs √ºbersprungen: {self.stats['skipped_external']}")
        logger.info(f"‚ö†Ô∏è  Ung√ºltige URLs √ºbersprungen: {self.stats['skipped_invalid']}")
        logger.info(f"üîê Login-Seiten gefunden: {self.stats['login_pages_found']}")
        logger.info(f"üîë Login-Versuche: {self.stats['login_attempts']}")
        logger.info(f"‚úÖ Erfolgreiche Logins: {self.stats['login_successes']}")
        logger.info(f"üéØ Crawl-Scope: {self.crawl_scope.value}")
        logger.info(f"üìÅ Ausgabeordner: {self.output_dir}")
        logger.info("=" * 60)
        
        # Erfolgsrate berechnen
        if self.stats['processed'] > 0:
            success_rate = (self.stats['processed'] / (self.stats['processed'] + self.stats['errors'])) * 100
            logger.info(f"‚úÖ Erfolgsrate: {success_rate:.1f}%")
        
        # Login-Erfolgsrate berechnen
        if self.stats['login_attempts'] > 0:
            login_success_rate = (self.stats['login_successes'] / self.stats['login_attempts']) * 100
            logger.info(f"üîê Login-Erfolgsrate: {login_success_rate:.1f}%")
        
        # Empfehlungen basierend auf Statistiken
        if self.stats['pagination_links_found'] > 0:
            logger.info(f"üí° Tipp: {self.stats['pagination_links_found']} Pagination-Links wurden erkannt und priorisiert verarbeitet.")
        
        if self.stats['skipped_external'] > 10:
            logger.info(f"üí° Tipp: {self.stats['skipped_external']} externe Links √ºbersprungen. Erw√§gen Sie Domain-Erweiterung falls n√∂tig.")
        
        if self.stats['login_pages_found'] > 0:
            logger.info(f"üí° Tipp: {self.stats['login_pages_found']} Login-gesch√ºtzte Seiten gefunden. Session-Cookies wurden f√ºr weitere Requests verwendet.")
        
        # Scope-spezifische Empfehlungen
        if self.crawl_scope == CrawlScope.ALL_URLS:
            logger.info(f"üí° Tipp: ALL_URLS Modus verwendet - alle gefundenen URLs wurden verarbeitet, unabh√§ngig von der Domain.")
        elif self.crawl_scope == CrawlScope.DOMAIN_ONLY:
            logger.info(f"üí° Tipp: DOMAIN_ONLY Modus - nur URLs der Domain {self.base_domain} wurden verarbeitet.")
        
        print(f"\nüéâ Crawling erfolgreich abgeschlossen!")
        print(f"üìä {self.stats['processed']} Seiten verarbeitet")
        print(f"üìÅ Dateien gespeichert in: {self.output_dir}")
        print(f"üìÑ Pagination-Links: {self.stats['pagination_links_found']}")
        print(f"üñºÔ∏è  Bilder: {self.stats['images_downloaded']}")
        print(f"üîó Links: {self.stats['total_links_found']}")
        print(f"üîê Login-Seiten: {self.stats['login_pages_found']}")
        print(f"üéØ Crawl-Scope: {self.crawl_scope.value}")
        if self.single_pdf:
            print(f"üìñ Fusioniertes PDF: Website_Komplett.pdf")
        print(f"üìã Logs: crawler.log")
        print(f"üìÇ Textdateien: /texts/ Ordner")
        print(f"üìä JSON-Berichte: url-collection.json & summary.json")

def main():
    """Hauptfunktion"""
    print("=== Website Crawler und Archiver ===")
    print("üî• Mit Login-Support, PDF-Fusion und flexiblen Crawling-Modi")
    print()
    
    # Debug-Modus abfragen
    debug_input = input("Debug-Modus aktivieren? (j/n, Standard: n): ").strip().lower()
    if debug_input in ['j', 'ja', 'y', 'yes']:
        enable_debug()
        print("üîç Debug-Modus aktiviert - detaillierte Logs werden angezeigt")
    
    # Benutzer-Eingaben
    base_url = input("Bitte geben Sie die Basis-URL ein: ").strip()
    if not base_url:
        print("Fehler: Keine URL angegeben!")
        return
    url_title = base_url.replace(".", "_").replace("/","_").replace("https://", "").replace("http://", "").replace("www.","")
    # URL-Format pr√ºfen
    if not base_url.startswith(('http://', 'https://')):
        base_url = 'https://' + base_url
    
    # Basis-Domain extrahieren f√ºr Anzeige
    parsed_url = urlparse(base_url)
    base_domain = parsed_url.netloc
    base_path = parsed_url.path
    
    # Crawling-Scope abfragen
    print(f"\nüéØ Crawling-Bereich f√ºr {base_url}:")
    print("=" * 60)
    print("1. üìÇ Hierarchisch (Standard)")
    print(f"   ‚îî‚îÄ Nur URLs unter/neben: {base_path}")
    print(f"   ‚îî‚îÄ Beispiel: {base_domain}{base_path}/subpage, {base_domain}{base_path}/../neighbor")
    print()
    print("2. üåê Domain-weit")
    print(f"   ‚îî‚îÄ Alle URLs der Domain: {base_domain}")
    print(f"   ‚îî‚îÄ Beispiel: {base_domain}/any/path, {base_domain}/different/section")
    print()
    print("3. üåç Unbeschr√§nkt")
    print("   ‚îî‚îÄ Alle gefundenen URLs, auch externe Domains")
    print("   ‚îî‚îÄ Beispiel: other-site.com, external-docs.org")
    print()
    print("‚ö†Ô∏è  Hinweis: Modus 3 kann sehr viele URLs finden - niedrigere max. Anzahl empfohlen!")
    print()
    
    scope_choice = input("W√§hlen Sie einen Modus (1/2/3, Standard: 1): ").strip()
    
    if scope_choice == '2':
        crawl_scope = CrawlScope.DOMAIN_ONLY
        scope_description = f"Domain-weit ({base_domain})"
    elif scope_choice == '3':
        crawl_scope = CrawlScope.ALL_URLS
        scope_description = "Unbeschr√§nkt (alle URLs)"
        print("‚ö†Ô∏è  ACHTUNG: Unbeschr√§nkter Modus ausgew√§hlt!")
        print("üí° Empfehlung: Niedrigere max. URL-Anzahl (10-50) verwenden")
    else:
        crawl_scope = CrawlScope.HIERARCHICAL
        scope_description = f"Hierarchisch ({base_path})"
    
    try:
        if crawl_scope == CrawlScope.ALL_URLS:
            default_max = "20"  # Niedrigere Standard-Anzahl f√ºr unbeschr√§nktes Crawling
        else:
            default_max = "100"
        
        max_urls = input(f"Wie viele URLs sollen maximal verarbeitet werden? (Standard: {default_max}): ").strip() or default_max
        max_urls = 999999 if max_urls in ["*", "-1", "0"] else int(max_urls)
    except ValueError:
        max_urls = int(default_max)
    
    # PDF-Option abfragen
    print("\nüìñ PDF-Ausgabe-Optionen:")
    print("1. Jede Seite als separate PDF-Datei")
    print("2. Alle Seiten in einem einzelnen PDF fusioniert")
    pdf_choice = input("W√§hlen Sie eine Option (1/2, Standard: 1): ").strip()
    
    single_pdf = pdf_choice == '2'
    
    # Best√§tigung
    print(f"\n‚öôÔ∏è  Einstellungen:")
    print(f"üìç Basis-URL: {base_url}")
    print(f"üéØ Crawling-Bereich: {scope_description}")
    print(f"üî¢ Max. URLs: {max_urls}")
    print(f"üìñ PDF-Modus: {'Einzelnes PDF' if single_pdf else 'Separate PDFs'}")
    print(f"üîç Debug-Modus: {'Aktiviert' if DEBUG_MODE else 'Deaktiviert'}")
    print(f"üîê Login-Support: Aktiviert (bei Bedarf wird nachgefragt)")
    print()
    
    # Scope-spezifische Warnungen
    if crawl_scope == CrawlScope.ALL_URLS:
        print("‚ö†Ô∏è  WARNUNGEN f√ºr unbeschr√§nkten Modus:")
        print("   ‚Ä¢ Kann sehr viele externe URLs finden")
        print("   ‚Ä¢ L√§ngere Laufzeit und h√∂herer Speicherverbrauch")
        print("   ‚Ä¢ Respektiert robots.txt anderer Domains m√∂glicherweise nicht")
        print("   ‚Ä¢ Bei Problemen mit Ctrl+C abbrechen")
        print()
    elif crawl_scope == CrawlScope.DOMAIN_ONLY:
        print("üí° Info: Domain-weiter Modus erfasst alle Bereiche von", base_domain)
        print()
    
    print("üìÅAusgabeordner:")
    print("    ‚îú‚îÄ‚îÄ images/          # Heruntergeladene Bilder")
    print("    ‚îú‚îÄ‚îÄ *.pdf            # PDF-Dateien")
    print("    ‚îú‚îÄ‚îÄ texts/           # Text-Dateien (.txt)")
    if single_pdf:
        print("    ‚îú‚îÄ‚îÄ Website_Komplett.pdf  # Fusioniertes PDF")
    print("    ‚îú‚îÄ‚îÄ url-collection.json   # Detaillierte URL-Sammlung")
    print("    ‚îú‚îÄ‚îÄ summary.json          # Kompakte Zusammenfassung")
    print("    ‚îú‚îÄ‚îÄ sitemap.txt           # Hierarchische Sitemap")
    print("    ‚îî‚îÄ‚îÄ crawler.log           # Detaillierte Logs")
    print()
    
    confirm = input("üöÄ Crawling starten? (j/n): ").strip().lower()
    if confirm not in ['j', 'ja', 'y', 'yes']:
        print("‚ùå Abgebrochen.")
        return
    
    # Hinweise f√ºr Login-gesch√ºtzte Seiten
    print("\nüí° Hinweise:")
    print("üîê Bei Login-gesch√ºtzten Seiten werden Sie nach Credentials gefragt")
    print("‚è≥ Zwischen Requests wird eine Pause eingelegt (h√∂fliches Crawling)")
    print("üõë Dr√ºcken Sie Ctrl+C um das Crawling zu stoppen")
    if crawl_scope == CrawlScope.ALL_URLS:
        print("üåç Unbeschr√§nkter Modus: Beobachten Sie die Logs auf unerwartete Domains")
    print()
    
    # Crawler starten
    try:
        crawler = WebsiteCrawler(base_url, max_urls, single_pdf, crawl_scope)
        crawler.crawl()
    except KeyboardInterrupt:
        print("\n‚è∏Ô∏è  Crawling wurde vom Benutzer abgebrochen.")
        print("üìä Bis hierhin gesammelte Daten wurden gespeichert.")
    except Exception as e:
        logger.error(f"Unerwarteter Fehler: {e}")
        print(f"‚ùå Fehler: {e}")
        if DEBUG_MODE:
            import traceback
            traceback.print_exc()

if __name__ == "__main__":
    main()